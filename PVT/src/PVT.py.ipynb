{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Importing Libraries\n",
        "###############################################################################\n",
        "############################# IMPORTING LIBRARIES #############################\n",
        "###############################################################################\n",
        "!pip install timm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from timm.models.vision_transformer import _cfg\n",
        "import math\n",
        "from PIL import Image\n",
        "print('Pytorch version: ', torch.__version__)\n",
        "print('GPU availability: ', torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "MVgwfF_m6OUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert Binaries to images\n",
        "os.mkdir(\"images\")\n",
        "os.mkdir(\"images/0\")\n",
        "os.mkdir(\"images/1\")\n",
        "# read binaries and convert to images\n",
        "def row2image(row):\n",
        "  img =list(row[1:1025]) \n",
        "  img = torch.Tensor(img).reshape(32,32)\n",
        "  label = row[1025:1026]\n",
        "  return img,label\n",
        "\n",
        "dataset = '/content/raw_pe_images.csv'\n",
        "nRowsRead = None # limit of rows\n",
        "\n",
        "df_dataset = pd.read_csv(dataset, delimiter=',', nrows = nRowsRead)\n",
        "df_dataset.dataframeName = 'malware_dataset'\n",
        "\n",
        "images = list(); labels = list()\n",
        "for i in range(1,51958):\n",
        "    row = df_dataset.iloc[i]\n",
        "    img,label = row2image(row)\n",
        "    images.append(img)\n",
        "    labels.append(label)\n",
        "# save images\n",
        "for i in range(49374,51957):\n",
        "  im = Image.fromarray((images[i]).numpy())\n",
        "  im = im.convert(\"L\")\n",
        "  im.save(\"/content/images/0/\"+str(i)+\".jpg\")\n",
        "\n",
        "for i in range(0,49373):\n",
        "  im = Image.fromarray((images[i]).numpy())\n",
        "  im = im.convert(\"L\")\n",
        "  im.save(\"/content/images/1/\"+str(i)+\".jpg\")\n",
        "\n",
        "\n",
        "# zip images and download them\n",
        "!zip -r /content/images.zip /content/images\n",
        "from google.colab import files\n",
        "files.download(\"/content/images.zip\")\n",
        "\n",
        "# show random images\n",
        "# images[0]\n",
        "f, ax = plt.subplots(1,3, figsize=(5, 5))\n",
        "ax[0].imshow((images[0].detach().cpu().numpy()))\n",
        "ax[1].imshow((images[51200].detach().cpu().numpy()))\n",
        "ax[2].imshow(images[51201].detach().cpu().numpy())\n",
        "\n",
        "# !git clone https://github.com/whai362/PVT.git"
      ],
      "metadata": {
        "id": "rWob4JZhdwOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., linear=False):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.dwconv = DWConv(hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.linear = linear\n",
        "        if self.linear:\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        x = self.fc1(x)\n",
        "        if self.linear:\n",
        "            x = self.relu(x)\n",
        "        x = self.dwconv(x, H, W)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1, linear=False):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.linear = linear\n",
        "        self.sr_ratio = sr_ratio\n",
        "        if not linear:\n",
        "            if sr_ratio > 1:\n",
        "                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
        "                self.norm = nn.LayerNorm(dim)\n",
        "        else:\n",
        "            self.pool = nn.AdaptiveAvgPool2d(7)\n",
        "            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1)\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "            self.act = nn.GELU()\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        if not self.linear:\n",
        "            if self.sr_ratio > 1:\n",
        "                x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "                x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
        "                x_ = self.norm(x_)\n",
        "                kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            else:\n",
        "                kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        else:\n",
        "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "            x_ = self.sr(self.pool(x_)).reshape(B, C, -1).permute(0, 2, 1)\n",
        "            x_ = self.norm(x_)\n",
        "            x_ = self.act(x_)\n",
        "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, linear=False):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio, linear=linear)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, linear=linear)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        \n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        \n",
        "        assert max(patch_size) > stride, \"Set larger patch_size than stride\"\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.H, self.W = img_size[0] // stride, img_size[1] // stride\n",
        "        self.num_patches = self.H * self.W\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
        "                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, H, W\n",
        "\n",
        "\n",
        "class PyramidVisionTransformerV2(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=8, in_chans=1, num_classes=2, embed_dims=[64, 128, 256, 512],\n",
        "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
        "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
        "                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], num_stages=4, linear=False):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.depths = depths\n",
        "        self.num_stages = num_stages\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        cur = 0\n",
        "\n",
        "        for i in range(num_stages):\n",
        "            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),\n",
        "                                            patch_size=7 if i == 0 else 3,\n",
        "                                            stride=4 if i == 0 else 2,\n",
        "                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n",
        "                                            embed_dim=embed_dims[i])\n",
        "\n",
        "            block = nn.ModuleList([Block(\n",
        "                dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer,\n",
        "                sr_ratio=sr_ratios[i], linear=linear)\n",
        "                for j in range(depths[i])])\n",
        "            norm = norm_layer(embed_dims[i])\n",
        "            cur += depths[i]\n",
        "\n",
        "            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n",
        "            setattr(self, f\"block{i + 1}\", block)\n",
        "            setattr(self, f\"norm{i + 1}\", norm)\n",
        "\n",
        "        # classification head\n",
        "        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            fan_out //= m.groups\n",
        "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def freeze_patch_emb(self):\n",
        "        self.patch_embed1.requires_grad = False\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n",
        "            block = getattr(self, f\"block{i + 1}\")\n",
        "            norm = getattr(self, f\"norm{i + 1}\")\n",
        "            x, H, W = patch_embed(x)\n",
        "            for blk in block:\n",
        "                x = blk(x, H, W)\n",
        "            x = norm(x)\n",
        "            if i != self.num_stages - 1:\n",
        "                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return x.mean(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DWConv(nn.Module):\n",
        "    def __init__(self, dim=768):\n",
        "        super(DWConv, self).__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.dwconv(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=16):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b0(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n",
        "        **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b1(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n",
        "        **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b2(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b3(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n",
        "        **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b4(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n",
        "        **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b5(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n",
        "        **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def pvt_v2_b2_li(pretrained=False, **kwargs):\n",
        "    model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], linear=True, **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QCGlmpZu3pwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining dataset\n",
        "###############################################################################\n",
        "############################# DEFINING DATASET ################################\n",
        "###############################################################################\n",
        "class MalwareDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = list()\n",
        "\n",
        "        for label in os.listdir(root):\n",
        "            subdir = os.path.join(root, label)\n",
        "            for im_name in os.listdir(subdir):\n",
        "                im = plt.imread(os.path.join(subdir, im_name))\n",
        "                self.images.append(im)\n",
        "\n",
        "        self.dataset_size = len(self.images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = self.transform(self.images[index])\n",
        "        if index > 49374:\n",
        "          lab = 0; \n",
        "        else: \n",
        "          lab=1\n",
        "        return im, lab\n",
        "\n",
        "    def show_random(self, num_samples=8):\n",
        "        indices = np.random.randint(0, len(self), [num_samples,])\n",
        "        f, ax = plt.subplots(num_samples, 2, figsize=(10, 10))\n",
        "        for i in range(num_samples):\n",
        "            noised_im, im = self[indices[i]]\n",
        "            ax[i, 0].imshow(im[0].numpy())\n",
        "            ax[i, 1].imshow(noised_im[0].numpy())\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "_bWM5KZOArHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# Hyper parameters\n",
        "################################################################\n",
        "BATCH_SIZE = 128\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "\n",
        "################################################################\n",
        "# Create training and testing dataset and show random examples\n",
        "################################################################\n",
        "trainval_set = MalwareDataset('/content/images', transform=transform)\n",
        "\n",
        "################################################################\n",
        "# As there is no validation set\n",
        "# We split training dataset into training and validation sets\n",
        "################################################################\n",
        "train_size = int(0.6 * len(trainval_set))\n",
        "val_size = int(0.2 * len(trainval_set))\n",
        "test_size = len(trainval_set) - train_size - val_size\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(\n",
        "    dataset=trainval_set,\n",
        "    lengths=[train_size, val_size,test_size],\n",
        "    generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "################################################################\n",
        "# Print lengths of subsets\n",
        "################################################################\n",
        "print('Training set size: ', len(train_set))\n",
        "print('Validation set size: ', len(val_set))\n",
        "print('Testing set size: ', len(test_set))\n",
        "\n",
        "################################################################\n",
        "# Print lengths of subsets\n",
        "################################################################\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset=val_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOXGh3p7A_rA",
        "outputId": "69a11fef-025f-490f-cc5e-656edbfb8fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size:  31173\n",
            "Validation set size:  10391\n",
            "Testing set size:  10392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#########################  INSTANTIATING THE MODEL  ############################\n",
        "################################################################################\n",
        "\n",
        "model = PyramidVisionTransformerV2(\n",
        "        patch_size=4, embed_dims=[32, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1])\n",
        "model.default_cfg = _cfg()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "uBjb9Xq06cMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#############################  TRAINING  ########################################\n",
        "################################################################################\n",
        "\n",
        "avg_loss_list_val = list()\n",
        "avg_loss_list_train = list()\n",
        "\n",
        "BestState = model\n",
        "model= model.cuda()\n",
        "best_acc = 0\n",
        "best_model = model\n",
        "\n",
        "for epoch in range(100):\n",
        "    well_pred = 0\n",
        "    model.train()\n",
        "    losses_train = list()\n",
        "    acc_list_train = list()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)\n",
        "        _, pred_label = torch.max(out, 1)\n",
        "        loss = criterion(out, target)\n",
        "        losses_train.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        well_pred += (target == pred_label).sum()\n",
        "    avg_loss_t = np.mean(losses_train)\n",
        "    avg_acc_t = well_pred/len(train_set)\n",
        "    avg_loss_list_train.append(avg_loss_t)\n",
        "    \n",
        "\n",
        "    # validation\n",
        "    well_pred = 0\n",
        "    losses_val = list()\n",
        "    model.eval()\n",
        "    acc_list_val = list()\n",
        "    for batch_idx, (x, target) in enumerate(val_loader):\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)\n",
        "        _, pred_label = torch.max(out, 1)\n",
        "        loss = criterion(out,target)\n",
        "        losses_val.append(loss.item())\n",
        "        well_pred += (target == pred_label).sum()\n",
        "    avg_loss_v = np.mean(losses_val)\n",
        "    avg_loss_list_val.append(avg_loss_v)\n",
        "    avg_acc_v = well_pred/len(val_set)\n",
        "\n",
        "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss_t:.6f}, val loss: {avg_loss_v:.6f}, train acc: {avg_acc_t:.6f}, val acc: {avg_acc_v:.6f}')\n",
        "\n",
        "    if avg_acc_v > best_acc:\n",
        "      best_loss = best_acc\n",
        "      BestState = model.state_dict()\n",
        "      torch.save(BestState, \"Model.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etQx4T3qJ4MN",
        "outputId": "c8095de8-acc7-4140-de0d-35e4d9d036ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==>>> epoch: 0, train loss: 0.209832, val loss: 0.193683, train acc: 0.945658, val acc: 0.951400\n",
            "==>>> epoch: 1, train loss: 0.184824, val loss: 0.173921, train acc: 0.949700, val acc: 0.951400\n",
            "==>>> epoch: 2, train loss: 0.170468, val loss: 0.165122, train acc: 0.949700, val acc: 0.951400\n",
            "==>>> epoch: 3, train loss: 0.160718, val loss: 0.167455, train acc: 0.949764, val acc: 0.951400\n",
            "==>>> epoch: 4, train loss: 0.151447, val loss: 0.155299, train acc: 0.949989, val acc: 0.951400\n",
            "==>>> epoch: 5, train loss: 0.139145, val loss: 0.168441, train acc: 0.951561, val acc: 0.950438\n",
            "==>>> epoch: 6, train loss: 0.130785, val loss: 0.165177, train acc: 0.953614, val acc: 0.947647\n",
            "==>>> epoch: 7, train loss: 0.113731, val loss: 0.163996, train acc: 0.957784, val acc: 0.947936\n",
            "==>>> epoch: 8, train loss: 0.100990, val loss: 0.182638, train acc: 0.962628, val acc: 0.945241\n",
            "==>>> epoch: 9, train loss: 0.084874, val loss: 0.199180, train acc: 0.969204, val acc: 0.933885\n",
            "==>>> epoch: 10, train loss: 0.072163, val loss: 0.208662, train acc: 0.975460, val acc: 0.939659\n",
            "==>>> epoch: 11, train loss: 0.049871, val loss: 0.233750, train acc: 0.983543, val acc: 0.935425\n",
            "==>>> epoch: 12, train loss: 0.038800, val loss: 0.223278, train acc: 0.986719, val acc: 0.943509\n",
            "==>>> epoch: 13, train loss: 0.033061, val loss: 0.269090, train acc: 0.989221, val acc: 0.944279\n",
            "==>>> epoch: 14, train loss: 0.040042, val loss: 0.250054, train acc: 0.987874, val acc: 0.941488\n",
            "==>>> epoch: 15, train loss: 0.024639, val loss: 0.341049, train acc: 0.992558, val acc: 0.931575\n",
            "==>>> epoch: 16, train loss: 0.020248, val loss: 0.241611, train acc: 0.993905, val acc: 0.947647\n",
            "==>>> epoch: 17, train loss: 0.017746, val loss: 0.296745, train acc: 0.994418, val acc: 0.945530\n",
            "==>>> epoch: 18, train loss: 0.014415, val loss: 0.308297, train acc: 0.995124, val acc: 0.943028\n",
            "==>>> epoch: 19, train loss: 0.015864, val loss: 0.312409, train acc: 0.995028, val acc: 0.947839\n",
            "==>>> epoch: 20, train loss: 0.015003, val loss: 0.299274, train acc: 0.995958, val acc: 0.946685\n",
            "==>>> epoch: 21, train loss: 0.012459, val loss: 0.338779, train acc: 0.996568, val acc: 0.943220\n",
            "==>>> epoch: 22, train loss: 0.012287, val loss: 0.332809, train acc: 0.996086, val acc: 0.946685\n",
            "==>>> epoch: 23, train loss: 0.017009, val loss: 0.308223, train acc: 0.994418, val acc: 0.943413\n",
            "==>>> epoch: 24, train loss: 0.014031, val loss: 0.343041, train acc: 0.996150, val acc: 0.945722\n",
            "==>>> epoch: 25, train loss: 0.006763, val loss: 0.379776, train acc: 0.997883, val acc: 0.948898\n",
            "==>>> epoch: 26, train loss: 0.008101, val loss: 0.331164, train acc: 0.997434, val acc: 0.944279\n",
            "==>>> epoch: 27, train loss: 0.010290, val loss: 0.330756, train acc: 0.997402, val acc: 0.941873\n",
            "==>>> epoch: 28, train loss: 0.014874, val loss: 0.343776, train acc: 0.995316, val acc: 0.932923\n",
            "==>>> epoch: 29, train loss: 0.011243, val loss: 0.337992, train acc: 0.996888, val acc: 0.942258\n",
            "==>>> epoch: 30, train loss: 0.011497, val loss: 0.330173, train acc: 0.996568, val acc: 0.944182\n",
            "==>>> epoch: 31, train loss: 0.008431, val loss: 0.353724, train acc: 0.997626, val acc: 0.933789\n",
            "==>>> epoch: 32, train loss: 0.007447, val loss: 0.319561, train acc: 0.997754, val acc: 0.944471\n",
            "==>>> epoch: 33, train loss: 0.007406, val loss: 0.350636, train acc: 0.997915, val acc: 0.945530\n",
            "==>>> epoch: 34, train loss: 0.009502, val loss: 0.356592, train acc: 0.996760, val acc: 0.937735\n",
            "==>>> epoch: 35, train loss: 0.021570, val loss: 0.380088, train acc: 0.993295, val acc: 0.942258\n",
            "==>>> epoch: 36, train loss: 0.010554, val loss: 0.319338, train acc: 0.996664, val acc: 0.949475\n",
            "==>>> epoch: 37, train loss: 0.006345, val loss: 0.357067, train acc: 0.997851, val acc: 0.945434\n",
            "==>>> epoch: 38, train loss: 0.004821, val loss: 0.415630, train acc: 0.998845, val acc: 0.947551\n",
            "==>>> epoch: 39, train loss: 0.005338, val loss: 0.360057, train acc: 0.998556, val acc: 0.945915\n",
            "==>>> epoch: 40, train loss: 0.005644, val loss: 0.319043, train acc: 0.998428, val acc: 0.942450\n",
            "==>>> epoch: 41, train loss: 0.005462, val loss: 0.386471, train acc: 0.998492, val acc: 0.946203\n",
            "==>>> epoch: 42, train loss: 0.009652, val loss: 0.342669, train acc: 0.997113, val acc: 0.944856\n",
            "==>>> epoch: 43, train loss: 0.007594, val loss: 0.285475, train acc: 0.997690, val acc: 0.947166\n",
            "==>>> epoch: 44, train loss: 0.009278, val loss: 0.372871, train acc: 0.997305, val acc: 0.943028\n",
            "==>>> epoch: 45, train loss: 0.006189, val loss: 0.335345, train acc: 0.998236, val acc: 0.939659\n",
            "==>>> epoch: 46, train loss: 0.008869, val loss: 0.305467, train acc: 0.997626, val acc: 0.947647\n",
            "==>>> epoch: 47, train loss: 0.007505, val loss: 0.327989, train acc: 0.997530, val acc: 0.946973\n",
            "==>>> epoch: 48, train loss: 0.005788, val loss: 0.257416, train acc: 0.998332, val acc: 0.946203\n",
            "==>>> epoch: 49, train loss: 0.005188, val loss: 0.337899, train acc: 0.998428, val acc: 0.947551\n",
            "==>>> epoch: 50, train loss: 0.002483, val loss: 0.410092, train acc: 0.999166, val acc: 0.947262\n",
            "==>>> epoch: 51, train loss: 0.001684, val loss: 0.399792, train acc: 0.999647, val acc: 0.947839\n",
            "==>>> epoch: 52, train loss: 0.005261, val loss: 0.312225, train acc: 0.998685, val acc: 0.941777\n",
            "==>>> epoch: 53, train loss: 0.012665, val loss: 0.312623, train acc: 0.995958, val acc: 0.941584\n",
            "==>>> epoch: 54, train loss: 0.008990, val loss: 0.362012, train acc: 0.997370, val acc: 0.939659\n",
            "==>>> epoch: 55, train loss: 0.007681, val loss: 0.351573, train acc: 0.997754, val acc: 0.944664\n",
            "==>>> epoch: 56, train loss: 0.005945, val loss: 0.378807, train acc: 0.998075, val acc: 0.942258\n",
            "==>>> epoch: 57, train loss: 0.005277, val loss: 0.308335, train acc: 0.998492, val acc: 0.940910\n",
            "==>>> epoch: 58, train loss: 0.004992, val loss: 0.380337, train acc: 0.998172, val acc: 0.950438\n",
            "==>>> epoch: 59, train loss: 0.005936, val loss: 0.391661, train acc: 0.998428, val acc: 0.946588\n",
            "==>>> epoch: 60, train loss: 0.010900, val loss: 0.268116, train acc: 0.996920, val acc: 0.946107\n",
            "==>>> epoch: 61, train loss: 0.006023, val loss: 0.374614, train acc: 0.998011, val acc: 0.946973\n",
            "==>>> epoch: 62, train loss: 0.003302, val loss: 0.412219, train acc: 0.998845, val acc: 0.943413\n",
            "==>>> epoch: 63, train loss: 0.004776, val loss: 0.384245, train acc: 0.998460, val acc: 0.945241\n",
            "==>>> epoch: 64, train loss: 0.003888, val loss: 0.383767, train acc: 0.998621, val acc: 0.950245\n",
            "==>>> epoch: 65, train loss: 0.006556, val loss: 0.341960, train acc: 0.998075, val acc: 0.945049\n",
            "==>>> epoch: 66, train loss: 0.005706, val loss: 0.393557, train acc: 0.998556, val acc: 0.945722\n",
            "==>>> epoch: 67, train loss: 0.003284, val loss: 0.391931, train acc: 0.998941, val acc: 0.948032\n",
            "==>>> epoch: 68, train loss: 0.004523, val loss: 0.351822, train acc: 0.998717, val acc: 0.949187\n",
            "==>>> epoch: 69, train loss: 0.002439, val loss: 0.408100, train acc: 0.999230, val acc: 0.949379\n",
            "==>>> epoch: 70, train loss: 0.005160, val loss: 0.341146, train acc: 0.998300, val acc: 0.948032\n",
            "==>>> epoch: 71, train loss: 0.008195, val loss: 0.361645, train acc: 0.997690, val acc: 0.946396\n",
            "==>>> epoch: 72, train loss: 0.004009, val loss: 0.367374, train acc: 0.998749, val acc: 0.949283\n",
            "==>>> epoch: 73, train loss: 0.002437, val loss: 0.403291, train acc: 0.999358, val acc: 0.946685\n",
            "==>>> epoch: 74, train loss: 0.002184, val loss: 0.380136, train acc: 0.999455, val acc: 0.943798\n",
            "==>>> epoch: 75, train loss: 0.006019, val loss: 0.410971, train acc: 0.998204, val acc: 0.945722\n",
            "==>>> epoch: 76, train loss: 0.005987, val loss: 0.375525, train acc: 0.998268, val acc: 0.946877\n",
            "==>>> epoch: 77, train loss: 0.004141, val loss: 0.335746, train acc: 0.998877, val acc: 0.946877\n",
            "==>>> epoch: 78, train loss: 0.005634, val loss: 0.345401, train acc: 0.998300, val acc: 0.949764\n",
            "==>>> epoch: 79, train loss: 0.006883, val loss: 0.378131, train acc: 0.997851, val acc: 0.947262\n",
            "==>>> epoch: 80, train loss: 0.007475, val loss: 0.369472, train acc: 0.997466, val acc: 0.946781\n",
            "==>>> epoch: 81, train loss: 0.002931, val loss: 0.428301, train acc: 0.999102, val acc: 0.942739\n",
            "==>>> epoch: 82, train loss: 0.002811, val loss: 0.414282, train acc: 0.999102, val acc: 0.947839\n",
            "==>>> epoch: 83, train loss: 0.003892, val loss: 0.439037, train acc: 0.998717, val acc: 0.948513\n",
            "==>>> epoch: 84, train loss: 0.001594, val loss: 0.464002, train acc: 0.999455, val acc: 0.945818\n",
            "==>>> epoch: 85, train loss: 0.001689, val loss: 0.440095, train acc: 0.999551, val acc: 0.948417\n",
            "==>>> epoch: 86, train loss: 0.005631, val loss: 0.379142, train acc: 0.998172, val acc: 0.945337\n",
            "==>>> epoch: 87, train loss: 0.009939, val loss: 0.374646, train acc: 0.997273, val acc: 0.942931\n",
            "==>>> epoch: 88, train loss: 0.006545, val loss: 0.318346, train acc: 0.998075, val acc: 0.946877\n",
            "==>>> epoch: 89, train loss: 0.002235, val loss: 0.389831, train acc: 0.999358, val acc: 0.942065\n",
            "==>>> epoch: 90, train loss: 0.003653, val loss: 0.343605, train acc: 0.998845, val acc: 0.947166\n",
            "==>>> epoch: 91, train loss: 0.001573, val loss: 0.384352, train acc: 0.999583, val acc: 0.949187\n",
            "==>>> epoch: 92, train loss: 0.001455, val loss: 0.380053, train acc: 0.999615, val acc: 0.945722\n",
            "==>>> epoch: 93, train loss: 0.001361, val loss: 0.400471, train acc: 0.999551, val acc: 0.946300\n",
            "==>>> epoch: 94, train loss: 0.001362, val loss: 0.355555, train acc: 0.999647, val acc: 0.949091\n",
            "==>>> epoch: 95, train loss: 0.001386, val loss: 0.399144, train acc: 0.999551, val acc: 0.947647\n",
            "==>>> epoch: 96, train loss: 0.001348, val loss: 0.352694, train acc: 0.999583, val acc: 0.947743\n",
            "==>>> epoch: 97, train loss: 0.001374, val loss: 0.417787, train acc: 0.999583, val acc: 0.949091\n",
            "==>>> epoch: 98, train loss: 0.001316, val loss: 0.417406, train acc: 0.999615, val acc: 0.947647\n",
            "==>>> epoch: 99, train loss: 0.001193, val loss: 0.451000, train acc: 0.999615, val acc: 0.947166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#############################  TESTING  ########################################\n",
        "################################################################################\n",
        "# torch.save(BestState, \"Model.txt\")\n",
        "checkpoint = torch.load(\"Model.txt\")\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "test_loss = list()\n",
        "\n",
        "model.eval()\n",
        "acc_list_test = list()\n",
        "losses_test = list()\n",
        "for batch_idx, (x, target) in enumerate(test_loader):\n",
        "      x, target = x.cuda(), target.cuda()\n",
        "      out = model(x)\n",
        "      _, pred_label = torch.max(out, 1)\n",
        "      loss = criterion(out,target)\n",
        "      losses_test.append(loss.item())\n",
        "      batch_acc = (target == pred_label).sum() / ((target == pred_label).sum() + (target != pred_label).sum())\n",
        "      acc_list_test.append(batch_acc.item())\n",
        "avg_loss_test = np.mean(losses_test)\n",
        "avg_acc_test = np.mean(acc_list_test)\n",
        "\n",
        "print(f'TESTING RESULTS ==>, Test loss: {avg_loss_test:.6f}, Test acc: {avg_acc_test:.6f}')\n",
        "       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sofy0DXJNh_m",
        "outputId": "ecaba3e6-ded6-4c9d-826b-7d4a5ceb57aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING RESULTS ==>, Test loss: 0.435017, Test acc: 0.949409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10,7)) \n",
        "plt.plot(avg_loss_list_train)\n",
        "plt.title('TRAINING LOSS')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend([ ''], loc='center' )\n",
        "plt.legend(['Training loss'], loc='center' )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "am_zp44X4avm",
        "outputId": "5f033883-3d3a-4ac2-def4-ce1c438c4afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4fd9abfa50>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}